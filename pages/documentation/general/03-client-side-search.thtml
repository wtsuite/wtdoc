import * as layout from "std/layout"
import * from "templates"
import {articlepage} from "index.thtml"


export var baseURL = ("https://developer.mozilla.org/en-US/docs/Web/JavaScript")


articlepage(2)
  append head
    style
      div#example-search-results {
        padding-left: 1.5rem;

        layout.flex(vlt);

        a {
          line-height: 2rem;
          height: 2rem;
        }
      }

  append intro
    p em "wtsuite" < " includes a special search indexer, and javascript search API. This search functionality is optimized for fast client-side search of small websites (&lt; 1000 pages). The API has partial and fuzzy search capabilities."
    br
    p "The std library provided page scoring isn't yet finished, and contributions are welcome."

  section
    h2 "Example"
    p "Type some search text into the input field below to search in the javascript documentation of " a(href=$baseURL) "MDN" < " (about 900 pages):"
    input(id=example-search-input, type=text)
    p(id=example-search-message)
    div(id=example-search-results)

    br
    p "The index used in this example is 5.4MB uncompressed, and 1.1MB gzipped."

  section
    h2 "Indexing local files"
    p "The indexing command is called " code "wt-search-index" < ". Looks for all html files (by extension!) in a given directory. The directory is assumed to represent the root of the website to be indexed."

    cblock
      cl "> wt-search-index ./www -c ./search-config.json -o ./www/search-index.json"

    br
    p "The " code "wt-search-index" < " command requires a config file. The config file tells the " code "wt-search-index" < " which tags to index and which words to ignore."
    br
    p "The output is javascript object, with information about each indexed page, and a special hierarchial search index enabling instant fuzzy search."
    br
    p "The output file will be roughly 2KB per page, so websites with more than about 1000 pages it is recommended to use server side search techniques."

    h3 "Well-formedness"
    p code "wt-search-index" < " uses its own xml parser. This xml parser is more strict than most browsers, and thus some pages parseable by a browser might be ignored by the indexer. (The indexer is mostly intended for the well-formed output generated by " em "wtsuite" < "'s transpilers.)"

  section
    h2 "Search config file"
    cblock
      cl "{"
      cl(1) '"title-query": "h1",'
      cl(1) '"content-query": "main",'
      cl(1) '"description": false,'
      cl(1) '"ignore": ["a", "all", "always", "an", "and", "any", "are", "at", "be", "but", "by", "can", "does", "for", "got", "has", "have", "if", "in", "into", "is", "it", "its", "like", "must", "no", "not", "of", "or", "our", "per", "that", "the", "their", "them", "there", "this", "to", "us", "use", "used", "uses", "want", "what", "where", "which", "will", "with", "would"]'
      cl "}"

    br
    p "Valid css selectors can be used for the " code "title-query" < " and " code "content-query" < "."
    br
    p "The meta description can optionally included as part of the content."

  section
    h2 "Indexing a live website"
    p "A live website must first be downloaded before it can be indexed. The " code "wt-crawl" < " command accomplishes this."
    br
    p code "wt-crawl" < " looks for link tags with a (clean) href attribute. All found files are downloaded into a directory recursively. This directory again represents the root of the website (or at least the part you want to index)."
    cblock
      cl "> wt-crawl https://computeportal.github.io/wtsuite-doc -o ./wtsuite-doc_www -j 8"
    br
    p "The " code "-j" < " option lets you parallelize the download and parsing process."
    br
    p "Note that " code "wt-crawl" < " is much slower than " code "wt-search-index" < " due to download latencies."

  section
    h2 "Search API"

    p "Import the std library module:"
    cblock
      cl ck "import" < " * " ck "as" < " search " ck "from" < " " cv "std/search" < ";"

    br
    p "Make sure the std library is defined in your " code "package.json" < " file."
    cblock
      cl "{"
      cl(1) '"dependencies": {'
      cl(2) '"std": {'
      cl(3) '"minVersion": "0.0.0",'
      cl(3) '"maxVersion": "2.0.0",'
      cl(3) '"url": "github.com/computeportal/wtlib"'
      cl(2) "}"
      cl(1) "}"
      cl(1) "..."
      cl "}"

    br
    p "Create a new " code "SearchIndex" < " object, giving the constructer the url from you will be hosting the " code "search-index.json" < " file:"
    cblock
      cl "searchIndex := " ck "new" < ' search.SearchIndex("/search-index.json");'

    br
    p "Updating search results as you type is easiest from within an " code "async" < " function:"
    cblock
      cl ck "private async" < " updateResults() {"
      cl(1) "query := " ck "this" < "._input.value;"
      cl
      cl(1) "keys := search.sanitizeQuery(query);"
      cl
      cl(1) "pages := " ck "await" < " search.searchPages(" ck "this" < "._searchIndex, keys, " cv "false" < ");"
      cl(1) "..."
      cl "}"

    br
    p "Use the " code "sanitizeQuery" < " function to turn a query string into an array of (clean) keywords."

    br
    p "The resulting " code "pages" < " are sorted according to an internal " code "PageScore" < ", and the first page in the array is deemed the most relevant."

    br
    p "Each page object in the array has a " code "title" < ", " code "url" < " (relative), and " code "content" < " property. The " code "content" < " property is an array of strings that you can use to display a short description of the relevant page."
    br
    p "Have a look at " a(href="/SearchExample.tjs") "this file" < " to see how we did the processing of the search results in the above example."
